---
title: "HW 2"
author: "Parijat"
date: "1/23/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library}
library(faraway)
library(ggplot2)
```

## 2.1

```{r}
X.lm = data.matrix(teengamb[,1:4])
X = cbind(X.lm,intercept=1)
Y = c(teengamb[,5])
beta.hat.lr = solve(t(X) %*% X) %*% t(X) %*% Y
model = lm(Y ~ X.lm)
#model$coefficients
```
Coefficents from lm:  
(Intercept)      X.lmsex   X.lmstatus   X.lmincome   X.lmverbal  
 22.55565063 -22.11833009   0.05223384   4.96197922  -2.95949350  

Coefficients from least squares estimates:  
    sex     status   income    verbal intercept  
 -22.11833 0.05223384 4.961979 -2.959493  22.55565  
 
 
 Coefficients match!!  

```{r}
scatter.smooth(y = model$residuals, x= model$fitted.values)
```

## 2.2

```{r}
n = 20
x = c(1:n)

Y = x + rnorm(n)

p = 5

X = c(intercept=1)


for(i in 1:p)
{
  x.i = x^(i)
  
  
  X = cbind(X,  x.i)
 
  model = lm(Y ~ X[,2:(i+1)])
 
  model_coeff = data.matrix(unname(unlist(model$coefficients)))
 
  beta2.lr = unname(solve(t(X) %*% X) %*% t(X) %*% Y)
  
  model_coeff = round(model_coeff,6)
  beta2.lr = round(beta2.lr,6)
  
  equal = all(model_coeff == beta2.lr)
  
  
  #print(paste(i,equal))

}
```


Fails for p = 6  

## 2.3

```{r}

#head(truck)
for(i in 1:5){
  
  truck[,i] = sapply(truck[,i], function(x) ifelse(x=="-",-1,1))
}

#head(truck)

X.truck = sapply(truck[,1:5],as.numeric)
Y.truck = sapply(truck$height,as.numeric)
model_truck = lm(Y.truck ~ X.truck)
#head(X.truck)
```


Regression Coeff:  

(Intercept)    X.truckB    X.truckC    X.truckD    X.truckE    X.truckO   
  7.6360417   0.1106250  -0.0881250  -0.0143750   0.0518750  -0.1297917   
  
```{r}
X = model.matrix(model_truck)
XtX = t(X) %*% X

```


The feature vectors are orthogonal and so calculation of each coeff is independent of another. $$X^T X = 48I  \rightarrow (X^T X)^{-1} = 1/48 I $$Therefore, $$beta.truckb = 1/48 X.truckB^T Y$$


```{r}
model.B = lm(Y.truck~X.truck[,1])
model.C = lm(Y.truck~X.truck[,2])
model.D = lm(Y.truck~X.truck[,3])
model.E = lm(Y.truck~X.truck[,4])
model.O = lm(Y.truck~X.truck[,5])

#model.B$coefficients
#model.C$coefficients
#model.D$coefficients
#model.E$coefficients
#model.O$coefficients
```

\textbf{coeffecients with B:}  
7.636042     0.110625  
\textbf{coeffecients with C:}  
7.636042    -0.088125  
\textbf{coeffecients with D:}  
7.636042    -0.014375  
\textbf{coeffecients with E:}  
7.636042     0.051875   
\textbf{coeffecients with O:}  
7.6360417   -0.1297917   

Since the full data  matrix is orthogonal, $X^T X = 48I  becomes (X^T X)^{-1} = 1/48 I $. Hence, $\beta = 1/48 X^TY$ which implies $\beta_i = X_i ^ T Y$. We get this same equation if we apply linear regression on each variable separately. $new \beta = (X^{'T}X^{'})^{-1}X^{'}Y$ where $$ X^{'} = \begin{bmatrix} 1  & X_i\end{bmatrix}$$
and $$(X^{'T}X^{'})^{-1} = \begin{bmatrix} 48 & 0 \\ 0 & 48 \end{bmatrix} $$

```{r}
#head(X.truck)
A = X.truck[,1] + X.truck[,2] + X.truck[,3] + X.truck[,4] + X.truck[,5]

X.truck = cbind(X.truck, A)

#head(X.truck)
model_truck.6 = lm(Y.truck ~ X.truck)
```
Coefficients:  

(Intercept)    X.truckB    X.truckC    X.truckD    X.truckE    X.truckO X.truckA  
  7.6360417   0.1106250  -0.0881250  -0.0143750   0.0518750  -0.1297917 NA  
  
No, coefficients do not appear for col A as it makes the matrix singular and it is no longer full rank.  

References:  
1. https://discuss.analyticsvidhya.com/t/linear-regression-in-r-coefficients-having-na-in-summary-model/64624/7
    
    